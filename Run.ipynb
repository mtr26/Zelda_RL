{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OXEorFbP_x2"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Maynerx/ZeldaRL_Ripo.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ZeldaRL_Ripo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r Rom/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from Rom.Zelda_env import *\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, VecMonitor, SubprocVecEnv, DummyVecEnv\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from Rom.SaveOnBestCallback import SaveOnBestTrainingRewardCallback\n",
        "from stable_baselines3.common import results_plotter\n",
        "from stable_baselines3.common.results_plotter import plot_results\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "def make_env(rank, seed=0):\n",
        "    \"\"\"\n",
        "    Utility function for multiprocessed env.\n",
        "    :param env_id: (str) the environment ID\n",
        "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
        "    :param seed: (int) the initial seed for RNG\n",
        "    :param rank: (int) index of the subprocess\n",
        "    \"\"\"\n",
        "    def _init():\n",
        "        env = ZeldaEnv(rank, save=False, speed=6)\n",
        "        env.reset(seed=(seed + rank))\n",
        "        return env\n",
        "    set_random_seed(seed)\n",
        "    return _init\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    #ep_length = 2048*2\n",
        "    timesteps = int(1.2e6)\n",
        "    #learn_steps = 5\n",
        "    num_cpu = 15\n",
        "    log_dir = \"tmp/\"\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    pre_trained = False\n",
        "\n",
        "    vec_env = SubprocVecEnv([make_env(i) for i in range(num_cpu)])\n",
        "\n",
        "\n",
        "    vec_env = VecFrameStack(vec_env, n_stack=4)\n",
        "\n",
        "    vec_env = VecMonitor(vec_env, log_dir)\n",
        "    callback = SaveOnBestTrainingRewardCallback(check_freq=4096, log_dir=log_dir)\n",
        "\n",
        "    \n",
        "    if pre_trained:\n",
        "        model = PPO.load('best_model', env=vec_env)\n",
        "        model.set_parameters('best_model')\n",
        "    else:\n",
        "        model = PPO('CnnPolicy', env=vec_env,  n_steps=2048, batch_size=512, n_epochs=1, gamma=0.999)\n",
        "        model.learn(total_timesteps=timesteps, progress_bar=True, callback=callback)\n",
        "        model.save('end_model')\n",
        "\n",
        "    plot_results([log_dir], timesteps, results_plotter.X_TIMESTEPS, \"ZeldaTest\")\n",
        "    plt.show()\n",
        "    \n",
        "    \"\"\"\n",
        "    for _ in range(learn_steps):\n",
        "        model.learn(total_timesteps=ep_length*num_cpu, progress_bar=False, callback=callback)\n",
        "        print(_)\n",
        "\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
